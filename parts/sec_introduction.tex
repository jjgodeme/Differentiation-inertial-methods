\section{Introduction}\label{sec-intro}

\subsection{Problem Statement \& Prior work}
In this paper, we study derivatives of an iterative process, in the ``automatic'' differentiation sense, to solve a parametric optimization problem. This approach is  one the most popular method to differentiate an  iterative algorithm generated by a computer.  Differentiating algorithms has been properly introduced in the 90's see  \cite{gilbert92,beck1994} for instance and gained recently a lot of attention within applications such as machine learning precisely hyperparameter optimization, metalearning \etc. We refer the interested reader to  \cite{griewank1993,griewank2007} for a more detailed introduction and to \cite{baydin2018} for a recent review to  \cite{iutzeler24} for the stochastic case, to \cite{bolte22} for an extension to the nonsmooth setting  and the following  thesis \cite{vaiter2021} and reference therein.

 It is well known that for a huge amount of smooth program, the derivatives can be obtained using  the rule of derivation of  composed functions  known as ``chain rule''. Therefore, the main goal is to know whether or not if we have convergence of the derivatives of the iterates to the derivative of the solution of the optimization problem. The standard approach in the smooth case is to use the well-known implicit function theorem which is our concern in this paper. To be more precise, we consider the following parametric-optimization problem 
\begin{equation}\label{eq:param-optim-intro}\tag{$\calP_{\theta}$}
\min_{x\in\bbR^n}f(x,\theta)
\end{equation}
where $\theta\in\Theta \subseteq\bbR^m$ is a parameter in the problem and the objective function is smooth enough. One of the most common way to solve this problem is to use the gradient-descent  like methods or accelerated version which are known to be inertial methods. The natural questions which come and are the core of our analysis are the following: 

\textit{Do we have convergence of the derivatives of the inertial methods to the derivatives of the solution? And if the answer is positive, can we quantify this convergence rates ?}

Despite its conceptual simplicity, the answers to these questions are not as trivial as it may seem and requires a  careful analysis before apply any differentiation toolboxes. We recall that our motivations for studying this questions come directly from the large range of  applications  as mentioned above. The second is that, they are no proper answer to this questions to the best of our knowledge. We hope that our tentative of answer clarifies the questions and propose a good theoretical mathematical framework and background to the daily users of  differentiation methods. 
%\subsection{Prior work}


%Automatic differentiation of algorithms is a concept at the interface between  applied mathematics and computer science, which cover the notion of differentiating algorithms. It has been theoretically introduced in the seminal  work \cite{griewank1989}. Before that, it was implemented from a numerical perspectives in two modes the forward mode \cite{wengert1964} and in reverse mode \cite{rumelhart1986} commonly called backpropagation. Roughly speaking, the forward mode is the natural or intuitive mode of differentiation at least from a mathematical point of view or simply calculus intuition. It consist in computing the derivative of the iterate using the  value of the immediate derivative of previous iterates. Whereas the reverse mode consist in computing and store a certain number of iterates before  apply a certain rule of derivation. Thus, this approach is limited by the memory space. This challenge can be avoided by using checkpointing approach such that  truncated backpropagation \cite{shaban2019}, which consist in taking a part of the stored derivative according to a certain probability distribution. The Jacobian-free backpropagation \cite{fung2022}, which consist instead of using the Jacobian direction, in choosing a direction according to an appropriate rule to compute the next derivative. The invertible optimization algorithms \cite{maclaurin2015}, which consist in inverting an appropriate linear map at each iteration to compute the derivative of the new iterate and many other approach.

%Approximate implicit differentiation (AID), combined with iterative differentiation (ITD) also known as implicit deep learning are crucial on solving some special type of learning problems. This allow for instance, to be able to highlights fixed-point equations  in defining hidden features \cite{elghaoui2021}. Whereas, it can be used to find an equilibrium for sequence models, reducing memory consumption significantly \cite{bai2019}. Moreover, one can extend this implicit differentiation's application to high-dimensional nonsmooth, convex  problems \cite{bertrand2020}. Concerning efficiency, it has been proven in \cite{ablin2020} that automatic differentiation performs  well especially in min-min optimization such as OptNet \cite{amos2017} and Deep Equilibrium Models \cite{bai2019}.

%Estimation of hypergradient using iterative differentiation or implicit differentiation is a well known procedure in the learning community see \cite{pedregosa15,lorraine20} for an account. For instance, it was used to perform selection of hyperparameter using the Stein's unbiased risk estimator  \cite{deladalle14,ochs19}  also called  SUGAR. It is based on Stein Lemma, which states that for a normally distributed variable $X$, the expected value $\esp{Xg(X)}=\esp{g'(X)}$  for any absolutely continuous (derivable a.e) such that $\esp{\abs{g'(X)}}<\infty$.
 %Moreover, it was also used to  perform a refitting procedure \cite{deladalle17}  named Clear.  Let us also mention, the smooth convex setting for which inexact oracles have been studied by \cite{nedic10,devolder14}.   In the same approach, \cite{arbel21} leverages inexact implicit differentiation and warm-start strategies to match the computational efficiency of oracle methods, proving effective in hyperparameter optimization. For the stochastic iterative and implicit differentiation, the literature  is more limited. In the stochastic setting, \cite{grazzi21,grazzi23,grazzi24} considered implicit differentiation, mostly as a stochastic approximation to solve the implicit differentiation linear equation or use independent copies for the derivative part. Additionally, the work \cite{ji21} provides a thorough convergence analysis for AID and ITD-based methods, proposing the novel stocBiO algorithm for enhanced sample complexity. 
 
 %To train neural network, in \cite{finn17}  the authors developed a Model-agnostic Meta-leraning (MAML)  which was adapded to implicit differentiation in \cite{rajeswaran19}.  Many existing results presented in the literature are qualitative and relate to nonconvex optimization \cite{solodov98,borkar09,doucet17,ramaswamy17,dieuleveut23}.  
 
 %These developments yields to a study of bilevel programming in machine learning \cite{franceschi18,grazzi20}. In the stochastic case, considering a unified stochastic gradient descent approaches one can improve convergence rates for stochastic nested problems, we refer the reader to \cite{chen2021} for a detailed account. In general, stochastic approaches for bilevel optimization sample different batches for the iterate and derivative recursions. Furthermore,\cite{dagreou22,dagreou24} introduce a novel framework allowing unbiased gradient estimates and variance reduction methods for stochastic bilevel optimization.

%Although this is not the initial focus of this work, the technical bulk of our arguments requires to rewrite our scheme as the sum of an iteration and a non-vanishing deterministic errors.  Such questioning around robustness to errors have existed for decades for instance in the stochastic approximation literature, see for example \cite{emoliev83,chen87} and reference therein. In the smooth framework and deterministic setting, \cite{mehmood24} apply unrolled or automatic differentiation to a time varying iterative algorithms. 








\subsection{Contributions}
In this work, we start by analyzing  the parametric optimization problem \eqref{eq:param-optim-intro} from a pure optimization perspective. We provide  conditions, on which the optimization problem has at least one solution and under strong convexity with respect to the first variable, \eqref{eq:param-optim-intro} has actually  a unique minimizer for all fixed $\theta\in\Theta$. We then consider a set of inertial methods, for solving smooth enough optimization problem. This inertial methods can be written in very concise form in a higher dimension by defining an appropriate mapping. We study the properties of this mapping, we show that this mapping is continuously differentiable, moreover it is also Lipschitz continuous. We turn to provide a  global convergence analysis to the unique minimizer of the problem under some mild premise on the inertial parameters and the stepsizes. To properly understand the behaviour of this method, we also analyze  the local convergence rate of this schemes and it turns out that this convergence rates is linear. Since, our main purpose of the paper is to differentiate this inertial method, we provide a clear and concise analysis of the derivative of the inertial methods with respect to the parameters. Firstly, we use the rule of derivation of composed functions to get an explicit formula of the derivative of the inertial methods. Then, we show that  since, the sequence globally converges to the minimizer thus under some premises that using a  standard technique  the sequence of the derivative also converges to the derivative of the limit point which is the minimizer of the problem. Finally, we provide a  convergence rates analysis of this phenomenon. Indeed, we show that we have a local linear convergence with an error term which tends to zero  as  the number of iteration $k$ tends to infty. The most important part is that we do not suppose the Lipschitz continuity of the second order derivative like most work in the literature of differentiation of algorithm for minimizing  $C^2-$smooth function. We think that this hypothesis was too restrictive and do not cover most of practical applications. 

\subsection{Paper organization}

We organized the rest of this paper  as follows. In Section~\ref{sec:opt-prblm}, we consider the parametric optimization as a pure optimization problem  and we provide a full analysis of the convergence, global and local behaviour of any sequence generated by our inertial method to the unique minimizer of the problem under some mild premise.  Section~\ref{sec:ad-ima}  is devoted to  the proper analysis of the  differentiation of our inertial methods for solving  the parametric-optimization problem. We  illustrate most of our results with numerical experiments in  Section~\ref{sec_numexp}. Finally, we postpone in Appendix some technicals proofs, remarks and a toolbox on  differentiation that we use throughout our paper.


