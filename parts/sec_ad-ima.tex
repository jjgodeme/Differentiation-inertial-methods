%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Differentiation of  inertial methods }\label{sec:ad-ima}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In this section, our main purpose is to differentiate the inertial methods for solving the parametric optimization problem \eqref{eq:param-optim} for any $\theta\in\Theta\subseteq\bbR^m$ is an open set. We pause here to recall some basics notion about differentiation. 

We recall that ``automatic'' differentiation is a method to compute the derivatives of an iterative process with respect to some parameter. Let us start by properly define what we consider to be an infinite iterative process. We refer the reader to \cite[Section~2]{beck1994} for a detailed exposure. 
\begin{definition}\label{def:codelist}An infinite iterative process is the given of the following triplet  $\calJ=\Ppa{\seq{\Phi_k},\Theta,\xo}$ where $\Theta\subseteq\bbR^m$ and 
we have 
\begin{enumerate}
\item $\xo:\Theta\to\bbR^n$  and $\Phi_k:\bbR^{n}\times\Theta\to\bbR^n,\forall k\in\bbN$ are continuously differentiable functions,
\item we have that the following iteration 
	\begin{equation}\label{eq:code-list} 
			\xkp(\theta)=\Phi_k(\xk(\theta),\theta),\quad \forall k\in\bbN,
	\end{equation}
	exist. 
\end{enumerate}
Moreover, if the sequence of function $\xk(\theta) \to \avx(\theta)$ then $\avx$ is called \textit{limit of the infinite iterative process}.
\end{definition} 
If we suppose that the sequence $\seq{\xk(\cdot)}$ is differentiable, the following intuitive question  for the consistency of the differentiation of  the infinite iterative process is: if the sequence $\seq{\xk(\theta)}$  converges to $\avx(\theta)$, do we have that the sequence of the derivatives $\seq{\partial_{\theta}\xk(\theta)}$ converge also  to the derivative of the limit $\partial_{\theta}\avx(\theta)$ \ie

\textit{Do we have that  $\xk(\theta) \to \avx(\theta)$ implies that $\partial_{\theta}\xk(\theta) \to \partial_{\theta}\avx(\theta)$?}  This is not a simple question and depends on the properties of the iteration map $\Phi_k$. Thus, we need the following definition.
\begin{definition}[Derivative stable]\label{def:der-sta} 
A sequence of differentiable function $\seq{\xk(\cdot)}$ is called \textit{derivative-stable} if and only if there exists a limit function $\avx(\theta)$ such that the following holds:
\begin{equation}\label{eq:der-sta}
\xk(\theta) \to \avx(\theta) \Longrightarrow \partial_{\theta}\xk(\theta) \to \partial_{\theta}\avx(\theta).
\end{equation}
\end{definition}
%One of the  most important question differentiation is to determine if the sequence of functions is derivative stable. 

In our setting, we want to investigate whether or not the sequence of inertial methods is derivative stable.   We recall that the sequence is given by the following iterations 
\begin{equation}\label{eq:rewrite-scheme2}
\begin{cases}
& X_{k+1}(\theta)=F_k(X_k,\theta),\quad \forall k\in\bbN,\\
& \ak\in[0,\abar], \abar\leq1, \bk\in[0,\bbar], \bbar\leq1\qandq 0<\gak<\frac{2}{L}.
\end{cases}
\end{equation}
where we have $\transp{X_k}=\begin{pmatrix}\xk\\\zk\end{pmatrix}=\begin{pmatrix}\xk\\\xkm\end{pmatrix}\in\bbR^{2n}$. 

Before our investigation, we have the following Lemma which proves that any sequence generated by Algorithm~\ref{eq:inertial-methods} in the form \eqref{eq:rewrite-scheme2} is an infinite iterative process.
\begin{lemma}\label{lem:def-inf} Let us consider the  inertial methods \eqref{eq:rewrite-scheme2}. Under Premise~\ref{prem_A}, if $X_0:\Theta\to\bbR^{2n}$ is a continuously differentiable function with respect to the parameter $\theta$ then the triplet $\calA\eqdef\Ppa{\seq{F_k},\Theta,X_0}$ is an infinite  iterative process according to Definition~\ref{def:codelist}.
\end{lemma}
\begin{remark}\label{rmk:def-inf} This Lemma may seem spare as first sight but it is important for the analysis of any sequence  generated by a computer program. Mainly, in which sense we consider the sequence of functions that are generated. In our setting, we properly defined what is called an infinite iterative process and Lemma~\ref{lem:def-inf} show that  the inertial methods that we consider in this work, under suitable premises on the the objective function, is an infinite iterative process. Finally, any analysis of functions generated by a computer program should follow this part in view to be  more precise and mathematically correct.
\end{remark}

\begin{proof}
The proof is straightforward. Indeed by hypothesis,  the set of parameter $\Theta$ is an open subset of $\bbR^m$, $X_0$ is continuously differentiable function  with respect to the parameter $\theta$ to the space $\bbR^n\times\bbR^n$ and Lemma~\ref{lem:LipFk} ensures that $\forall k\in\bbR^n, F_k$ is continuously differentiable over the space $\bbR^n\times\bbR^n\times\Theta$.  Algorithm~\ref{eq:inertial-methods} in the form \eqref{eq:rewrite-scheme2} ensures that the sequence of functions generated is in the form \eqref{eq:code-list}. We finally conclude  that  the triplet $\calA$ is an infinite  iterative process according to Definition~\ref{def:codelist}.
\end{proof}

Let us denote  $\forall k\in\bbN, \partial_{\theta}X_k(\theta)\in \bbR^{2n\times m}$  to be  the Jacobian of $X_k(\theta)$ with respect to the parameter $\theta$. By the rule of derivation of composed functions, we have 
\begin{equation}\label{eq:pdcf-rule-in}
 \forall k\in\bbN, \quad \partial_{\theta}X_{k+1}(\theta)=J_1F_k(X_k(\theta),\theta)\partial_{\theta}X_k(\theta)+J_2F_k(X_k(\theta),\theta),
\end{equation}
where we have from \eqref{eq:Jac-xz} and \eqref{eq:Jac-theta} that 
\[
	 	J_1F_k(X_k\pa{\theta},\theta)= \begin{pmatrix} (1+\ak)\Id-\gak(1+\bk)\nabla_x^2f(\ybk(\xk,\xkm),\theta) &-\ak\Id+\gak\bk\nabla_x^2f(\ybk(\xk,\xkm),\theta) \\ \Id  & 
		0\end{pmatrix},
\]
and 
\[
		 	J_2F_k(X_k\pa{\theta},\theta)= \begin{pmatrix} -\gak\nabla_{x\theta}^2f(\ybk(\xk,\xkm),\theta)  \\  
		0\end{pmatrix}.
\]

\begin{remark}\label{rmk:inert-deriv}
 We  can observe from \eqref{eq:pdcf-rule-in} that the sequence of derivatives $\seq{\partial_{\theta}X_{k}(\theta)}$ is linear in $\partial_{\theta}X_{k}(\theta)$ with a perturbation  term which is represented by the function  $J_2F_k(X_k\pa{\theta},\theta)$. This simply  means  that  after  the differentiation  procedure of  inertial methods  the sequence of derivatives has no oscillations in term of $\partial_{\theta}X_{k}(\theta)$. As we may observe later in the numerical experiments. 
\end{remark}




\subsection{Convergence result of the derivative of the IM algorithm}	\label{subsec:gim-stable}
The following is our main convergence result of the derivative of the  inertial methods.
\begin{theorem}\label{thm:gim-stable} Let us consider the  inertial method Algorithm~\ref{eq:inertial-methods} in the form \eqref{eq:rewrite-scheme2} for minimizing the parametric optimization problem \eqref{eq:param-optim}, for each $ \theta\in\Theta$ . Under Premise~\ref{prem_A} on the objective function $f$ with the additional hypothesis that $f$ is strongly convex with respect to $x$, Premise \ref{prem_B} and \ref{prem_C} on the parameters of the Algorithm~\ref{eq:inertial-methods}  and that $X_0:\Theta\to\bbR^{2n}$ is a continuously differentiable function with respect to the parameters $\theta$ then, the following holds:
\begin{enumerate}
\item\label{thm:gim-stable1} the sequence of derivatives $\seq{\partial_{\theta}X_{k}\pa{\theta}}$ converges pointwise to the derivative of  $\partial_{\theta}X^{\star}\pa{\theta},$ 
\item\label{thm:gim-stable2} moreover, we have the following explicit  formula:  $\forall\theta\in\Theta,$
\begin{centerbox}{black}{}y
\small{
\begin{equation}\label{eq:expli-form}
 \hspace{-0.8cm}\partial_{\theta}X^{\star}\pa{\theta}=\begin{pmatrix} -a\Id+\gamma(1+b)\nabla_x^2f(\xsol(\theta),\theta) &a\Id-\gamma b\nabla_x^2f(\xsol\pa{\theta},\theta) \\ -\Id  & 
		\Id\end{pmatrix}^{-1}\hspace{-0.2cm}\begin{pmatrix} -\gamma\nabla_{x\theta}^2f(\xsol,\theta)  \\  
		0\end{pmatrix}.
\end{equation}}
\end{centerbox}

\end{enumerate}
\end{theorem}
\begin{remark} Clearly,  the claim~\ref{thm:gim-stable1} of this  theorem tell us that the  any sequence generated by our inertial methods Algorithm~\ref{eq:inertial-methods} in the form \eqref{eq:rewrite-scheme2}  under our hypothesis is derivative stable. Moreover, we have an explicit formula of the corresponding derivative in claim~\ref{thm:gim-stable2} and given by \eqref{eq:expli-form}.
\end{remark}
\begin{proof}$~$ 
We refer the reader to Section~\ref{prf:thm:gim-stable}.
\end{proof}

\subsection{Convergence rates of the derivative }\label{subsec:conv-ad-im}
In this section, we try to answer the question at which rates the convergence in Theorem~\ref{thm:gim-stable} occurs? This question may seem not important at first sight but is crucial to examine the behaviour of the derivative sequence. Clearly, we want to quantify this rates of convergence. We are ready now to state our main result on the convergence rate of the derivatives of the proposed inertial method Algorithm~\ref{eq:inertial-methods}.
\begin{theorem}[Convergence rate]\label{thm:conv-rat-der} Let us consider the inertial method Algorithm~\ref{eq:inertial-methods} in the form \eqref{eq:rewrite-scheme2} for minimizing the parametric optimization problem \eqref{eq:param-optim}, $ \theta\in\Theta$ . 

Under Premise~\ref{prem_A} on the objective function $f$, with the additional hypothesis that $f$ is strongly convex with respect to $x$, Premise \ref{prem_B} and \ref{prem_C} on the parameters of the Algorithm~\ref{eq:inertial-methods}  and that $X_0:\Theta\to\bbR^{2n}$ is a continuously differentiable function with respect to the parameters $\theta$ then for $\eps>0$ (small enough) their exists $K$ large enough such that:
\begin{centerbox}{black}{}y

\begin{multline}\label{thm:conv-rat-der}
\hspace{-1cm}\forall k\geq K, \normm{\partial_{\theta}X_{k+1}(\theta)-\partial_{\theta}X^{\star}(\theta)}\leq 2\rho\pa{M}\normm{\partial_{\theta}X_k(\theta)-\partial_{\theta}X^{\star}(\theta)}+\eps\Ppa{2+\normm{\partial_{\theta}X_k(\theta)-\partial_{\theta}X^{\star}(\theta)}}.
\end{multline}

\end{centerbox}
\end{theorem}

\begin{remark}\label{rem:conv-rat-der}Let us emphasize  that this result is very new to the best of our knowledge compared  to other works which quantify the convergence rate under an additive hypothesis such that Lipschitz continuity of the second order derivatives of the  objective  $f$. This additive hypothesis is quite restrictive in practice and does not apply to  lot of standard problems. We show ``almost'' the same result without this hypothesis. Therefore, our result has the advantage to be applicable to a broader set of problems.  Indeed, \eqref{thm:conv-rat-der} shows that the local linear convergence occurs in the long term regime of the sequence with a small additive error term. We observe that this error terms vanishes as the number of iterations increases and tends to infinity. \textit{Moreover, we would like to mention to the most curious reader that this result goes far beyond the particular case of  inertial methods that we study in this paper, we think that  it holds true for any infinite iterative process which satisfies the standard Premise~\ref{prem_apend} since in the proof we never used the explicit form of the quantity involved.} 
\end{remark}


\begin{proof}$~$

The proof of this Theorem can be found in Section~\ref{prf:thm:conv-rat-der}.
\end{proof}


We get the following corollary which represent  the convergence rate and can be useful in applications. 
\begin{corollary}
Let us  define 
\[
\tau\eqdef 2\rho(M),\quad  \forall k\in\bbN, \quad g(X_k)\eqdef 2+\normm{\partial_{\theta}X_k(\theta)-\partial_{\theta}X^{\star}(\theta)}.
\]
Under the same hypothesis  as Theorem~\ref{thm:conv-rat-der} then for $\eps>0$ (small enough) their exists $K$ large enough such that:
\begin{centerbox}{black}{}y
\begin{multline}\label{cor:conv-rat-der}
\forall k\geq K,  \normm{\partial_{\theta}X_{k+1}(\theta)-\partial_{\theta}X^{\star}(\theta)}\leq \tau^{k+1-K}\normm{\partial_{\theta}X_K(\theta)-\partial_{\theta} X^{\star}(\theta)}+\eps\sum_{i=K}^k\tau^{i-K}g(X_i).
\end{multline}
\end{centerbox}
\end{corollary}
\begin{remark}$~$
\begin{itemize}

\item Let us  mention that our convergence rate is different from existing convergence rate in the literature of differentiation. Our convergence rate in this work is $\tau$  with an vanishing error term while work around like \cite{mehmood19}, convergence rate are usually of the form  $\max\Ba{\rho(M),q_x}$ where $q_x$ is said to be the convergence rate of the sequence $\seq{\xk(\theta)}$ and without the error term in \eqref{cor:conv-rat-der}  which is quite important to notice. 
\item We  want to answer the question why our convergence rates is so different from previous work ? Our results rely on a careful analysis of the problem without bounding  the second order derivative from the beginning. We mean by ``careful analysis'' of the problem take into account all the premises of the problem without adding additional hypothesis which in this case is superfluous.
\end{itemize}
\end{remark}
\begin{proof}$~$This result is a  consequence of the previous Theorem by summing from  $K$ to $k$. 
\end{proof}

















