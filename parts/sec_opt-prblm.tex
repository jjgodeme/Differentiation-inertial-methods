%\section{Introduction}
%In this paper, we study derivatives of an iterative algorithm, in the automatic differentiation sense, to solve a parametric optimization problem. This approach is  one the most popular method to differentiate an infinite code list or iterative algorithms generated by a computer. This topic of automatic differentiation has been properly introduced for instance and gained recently a lot of attention within applications such as machine learning precisely hyperparameter optimization, metalearning and learning  discretization of the Total variation. It is well known that for a large  group of smooth program, the derivatives can be obtained using  the principe of derivation of  composed functions  known as ``chain rule'' . Therefore, the main goal is to know whether or not if we have convergence of the derivatives of the iterates to the derivative of the solution of the optimization problem. 

%\todo{Sorry, I have to finish writing this part! }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{ Optimization problem of interest}\label{sec:opt-prblm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Existence result}\label{subsec:exist}
We recall  that we want to solve the following optimization problem \eqref{eq:param-optim}
\begin{equation*}\tag{$\calP_{\theta}$} \label{eq:param-optim}
\min_{x\in\bbR^n}f(x,\theta)
\end{equation*}
where $\theta\in\Theta \subseteq\bbR^m.$  We have the following premises on the function that we want to optimize. 

\begin{premise}{\ }\label{prem_A}
\begin{enumerate}[label=(A.\arabic*)]
 \item \label{prem_A1} $f$ is a $C^2-$smooth  function with respect to both $x,\theta$,
 \item\label{prem_A2}  the gradient $\nabla_x f$ is $L-$Lipschitz continuous in both $x,\theta$, 
 \item \label{prem_A3}  $\forall\theta\in\Theta,$  the function  $x \mapsto f(x,\theta)$ is  level bounded \ie  all its sublevel sets are bounded. 
\end{enumerate}
\end{premise}
\begin{remark}
Premise~\ref{prem_A1} is a bear  requirement  since our main target is to differentiate a first-order method in the whole space. While Premise~\ref{prem_A2} implies that the operators $\nabla_{xx}^2f$ and $\nabla_{x\theta}^2f$  have upper bounds at each points. Finally, Premise~\ref{prem_A3} ensures that the minimization problem \eqref{eq:param-optim} has at least one minimizer for each $\theta\in\Theta$. It corresponds to having the property that $\forall \theta\in\Theta, \quad f(x,\theta)\to\infty$ as $\normm{x}\to\infty.$ 
\end{remark}


As an unconstraint optimization problem, the next question that arises is whether or not this problem has a solution. It turns out that under our given Premise~\ref{prem_A}, the answer is true. 
\begin{lemma}[Existence]\label{lem:exist} Let us consider  the problem \eqref{eq:param-optim} forall $\theta\in\Theta.$ Under the Premise~\ref{prem_A}, we have
\begin{enumerate}
\item\label{lem:exist_1} $\forall\theta\in\Theta,$ the minimization problem \eqref{eq:param-optim}  has a solution \ie $\argmin_x f(x,\theta)\neq\emptyset$. 
\item\label{lem:exist_2} Moreover,  if $\forall\theta\in\Theta,$ the function $x\mapsto f(x,\theta)$ is strongly convex then $\argmin_x f(x,\theta)$ is a singleton. 
\end{enumerate}
\end{lemma} 
\begin{proof}$~$
 Forall $\theta\in\Theta,$ \eqref{eq:param-optim} is an optimization problem and thanks to Premise~\ref{prem_A1}, $f(x,\theta)$ is continuous with respect to $x$. Moreover, under Premise~\ref{prem_A2},  for any $\theta\in\Theta$ the function $x\mapsto f(x,\theta)$ is level bounded. We conclude by applying \cite[Theorem~1.6]{rockafellar_variational_1998} which prove Claim~\ref{lem:exist_1}.
 The second Claim is a standard result in convex optimization and we refer to \cite{rockafellar_convex_1970} for an extensive study of the problem.
\end{proof}






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Solving with inertial methods}\label{subsec:solv-im}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



Throughout this section, we suppose that $f$ satisfy the Premise~\ref{prem_A} and that $\forall \theta\in\Theta,\quad x\mapsto f(x,\theta)$ is strongly convex. Thanks to Lemma~\ref{lem:exist}, for each $\theta\in\Theta$, \eqref{eq:param-optim} has a unique minimizer let us denote $\xsol(\theta)$. 
 
 Now, we want to solve \eqref{eq:param-optim} using an  inertial  methods  for smooth function  forall $\theta\in\Theta.$  
 This inertial methods was introduced in \cite{liang_activity_2017} and named inertial forward-backward methods. The goal was to solve the minimization of the sum of two functions where one is sufficiently smooth and the second is nonsmooth. Here, we suppose that the nonsmooth part is zero.
 The scheme generates the following sequence $\forall k\in\bbN$,
 \begin{algorithm}[htbp]
	\caption{ Inertial Methods}
	\label{eq:inertial-methods}
	\textbf{Parameters:}  $\ak\in[0,\abar], \abar\leq1,  \bk\in[0,\bbar], \bbar\leq1,0<\gak<\frac{2}{L}$\;
	\textbf{Initialization:} $x_{-1}=\xo\in\mathbb{R}^n$, $b_0=a_0=1$,  \;
	\For{$k=0,1,\ldots$}{
		\vspace{-0.25cm}
		\begin{flalign} \tag{IM}\label{BPGalgo} 
			&\begin{aligned} 
				&\yak(\xk,\xkm)=\xk+\ak\pa{\xk-\xkm}; \\
				&\ybk(\xk,\xkm)=\xk+\bk\pa{\xk-\xkm}; \\
				&\xkp=\yak(\xk,\xkm)-\gak\nabla f(\ybk(\xk,\xkm),\theta); \\ 
				%&\text{Choose }  a_{k+1}\in [\ua,\oa] \qobjq{s.t.} (a_{k+1}^{1-\kappa}+1)^{1/\kappa}(1-a_{k+1}) < a_k^{1/\kappa-1}/(1-a_k) . 
			\end{aligned}&
		\end{flalign}
	}
\end{algorithm}
 
 
 
 
 
%\begin{equation}
%\begin{cases}
%&\yak(\xk,\xkm)=\xk+\ak\pa{\xk-\xkm},\quad \ak\in[0,\abar], \abar\leq1\\
%&\ybk(\xk,\xkm)=\xk+\bk\pa{\xk-\xkm},\quad \bk\in[0,\bbar], \bbar\leq1\\
% &\xkp=\yak(\xk,\xkm)-\gak\nabla f(\ybk(\xk,\xkm),\theta),\quad  0<\gak<\frac{2}{L}.
%\end{cases}
%\end{equation}

This group of inertial algorithms \eqref{eq:inertial-methods} covers standard optimization methods such as:
\begin{itemize}
\item Gradient descent with the choice $\forall k\in\bbN, \ak=\bk=0,$
\item  Heavy-ball method \cite{polyak_methods_1964} with the choice $\forall k\in\bbN, \ak\in [0,\ava]$ and  $\bk=0,$
\item Nesterov's type methods \cite{nesterov_method_1983}  with the choice $\forall k\in\bbN, \ak\in [0,\ava], \bk=\ak \quad \st\quad \ak\to1$ and $\gak\in]0,1/L[$. It is common in this setting to choose $\forall k\in\bbN, \ak=\frac{k-1}{k+3}$ to ensure the  convergence of the iterates.
\end{itemize}
A common  way to properly  analyze this system is to increase the dimension of the problem from $\bbR^n$ to $\bbR^n\times\bbR^n$ by introducing a new variable $\zk=\xkm$. Thus, we get the following iteration 
\begin{equation}\label{eq:nest-system}
\begin{cases}
\xkp=&\yak(\xk,\zk)-\gak\nabla f(\ybk(\xk,\zk),\theta),\\
\zkp=&\xk.
\end{cases}
\end{equation}
Let us define, the following mapping
\begin{equation}\label{eq:mapping}
\forall k\in\bbN, \quad F_k(X,\theta)\eqdef F_k(x,z,\theta)=\begin{pmatrix}
\yak(x,z)-\gak\nabla f(\ybk(x,z),\theta)\\
x
\end{pmatrix},
\end{equation}
where $\transp{X}=\begin{pmatrix}x\\z\end{pmatrix}$. We have the following regularity on  $\seq{F_k
}$ which is stated in the following Lemma. 
\begin{lemma}\label{lem:LipFk} Under our Premise~\ref{prem_A} on the objective function $f$, we have  
\begin{enumerate}
\item $\forall k\in\bbN$, $F_k$ is a $C^1-$smooth function over the space $\bbR^n\times\bbR^n\times\Theta.$  
Moreover, the  Jacobian with respect to $(x,z)$ is
 	\begin{equation}\label{eq:Jac-xz}
	 	J_1F_k(x,z,\theta)\eqdef \begin{pmatrix} (1+\ak)\Id-\gak(1+\bk)\nabla_x^2f(\ybk(x,z),\theta) &-\ak\Id+\gak\bk\nabla_x^2f(\ybk(x,z),\theta) \\ \Id  & 
		0\end{pmatrix},
	\end{equation}
	while the Jacobian with respect to $\theta$ is
	\begin{equation}\label{eq:Jac-theta}
	 	J_2F_k(x,z,\theta)\eqdef \begin{pmatrix} -\gak\nabla_{x\theta}^2f(\ybk(x,z),\theta)  \\  
		0\end{pmatrix}.
	\end{equation}
\item $\forall k\in\bbN$, $F_k$ is also a Lipschitz continuous function with 
\begin{equation}\label{eq:L_F_k}
L_{F_k}= \sqrt{\Ppa{1+\pa{1+\ak}^2+(\gak L)^2\pa{1+\bk}^2}}.
\end{equation}
\end{enumerate}
\end{lemma}
\begin{proof}
We refer the reader to Section \ref{prf:lem:LipFk}.
\end{proof}
We can easily rewrite our inertial scheme Algorithm~\ref{eq:inertial-methods} as follow: $\forall k \in\bbN,\forall\theta\in\Theta$
\begin{equation}\label{eq:rewrite-scheme}
\begin{cases}
& X_{k+1}(\theta)=F_k(X_k,\theta),\\
& \ak\in[0,\abar], \abar\leq1, \bk\in[0,\bbar], \bbar\leq1\qandq 0<\gak<\frac{2}{L}.
\end{cases}
\end{equation}
where we have $\transp{X_k}=\begin{pmatrix}\xk\\\zk\end{pmatrix}$. 
\subsection{Convergence and local analysis}\label{subsec:conv-loc}

The next important questions are, if we Algorithm~\ref{eq:inertial-methods} to solve our optimization problem \eqref{eq:param-optim}, do we have convergence to the unique global minimizer $\xsol(\theta)$ as $k\to\infty$ ? Can we quantify the convergence rate ? In \cite{liang_activity_2017}, the authors  study and analyze the global and the local convergence of the  inertial forward-backwards methods. They provide conditions on the choice the inertial parameter $\seq{\ak},\seq{\bk}$ and  the stepsize $\seq{\gak}$ such that the generated sequence $\seq{\xk}$ converges to $\xsol$. In the next proposition, we specialize their result to our simpler case.   
We pause here,  to make the following premise on the choice of the stepsizes $\seq{\gak}$ and the inertials parameters $\seq{\ak},\seq{\bk}$.
\begin{premise}\label{prem_B} We suppose that there exists a constant $\tau>0$ such that one of the following holds
\begin{itemize}
\item $\forall k\in \bbN,\quad\tau<(1+\ak)-\frac{\gak L}{2}\pa{1+\bk}^2:\ak<\frac{\gak L}{2}b_k$
\item $\forall k\in \bbN,\quad\tau<(1-3\ak)-\frac{\gak L}{2}\pa{1-\bk}^2:\bk\leq\ak \qorq \frac{\gak L}{2}b_k\leq\ak<\bk.$
\end{itemize}
\end{premise}
\begin{remark}Here are two examples of inertial parameters which satisfy Premise~\ref{prem_B}, which can be found in  \cite[Section~5]{liang_activity_2017}. 
\begin{itemize}
\item \textbf{Example 1:} $\forall k\in\bbN, \gak=1/L, \ak=\bk =\sqrt{5}-2-10^{-3},$
\item \textbf{Example 2:} $\forall k\in\bbN, \gak=1/L, \ak=\bk =\frac{k-1}{k+25}.$
\end{itemize}
\end{remark}
The result is stated in the following proposition.
\begin{proposition}[Global convergence of the iterates] \label{pro:conv-iters} Let us  consider, the optimization problem \eqref{eq:param-optim} for all $\theta\in\Theta$ solved with the inertial method \eqref{eq:inertial-methods}. Under the premise~\ref{prem_A}, premise~\ref{prem_B} and the strong convexity hypothesis with respect to $x$, we have that the generated sequence $\seq{\xk(\theta)}$ has finite length and the sequence $\seq{\xk(\theta)}$ converges to $\xsol(\theta)$. 
\end{proposition}
\begin{proof}
We refer the interested reader to \cite[Section~A]{liang_activity_2017} for a detailed proof of this proposition.
\end{proof}
\begin{remark}\label{rem:conv-iters}$~$
\begin{itemize}
\item This proposition is a particular case of \cite[Theorem~4]{liang_activity_2017}, here we take the nonsmooth part to be zero.
\item Since the inertial methods  can be rewritten as the scheme \eqref{eq:rewrite-scheme} in $\bbR^n\times\bbR^n$, Proposition~\ref{pro:conv-iters} entails that the sequence $\seq{X_k(\theta)}$ converges to a unique point $X^{\star}(\theta )$ where $\transp{X^{\star}(\theta )}\eqdef\begin{pmatrix}\xsol(\theta)\\\xsol(\theta)\end{pmatrix}.$ 
\end{itemize}
\end{remark}

\paragraph*{Local behavior of the IM algorithm} $~$

Let us recall that thanks to Lemma~\ref{lem:LipFk}, $\forall k\in\bbN,$ the function $F_k$ defined in \eqref{eq:mapping} is $C^1-$smooth on $\bbR^n\times\bbR^n\times\Theta$ and set $\forall k\in\bbN,$
\begin{align*}\label{eq:Jsol}
 M_k\eqdef& J_1F_k(\xsol,\xsol,\theta)\\
=& \begin{pmatrix} (1+\ak)\Id-\gak(1+\bk)\nabla_x^2f(\xsol,\theta) &-\ak\Id+\gak\bk\nabla_x^2f(\xsol,\theta) \\ \Id  & 
		0\end{pmatrix}.\numberthis
\end{align*}
For any $a,b\in[0,1]$ and $\gamma\in]0,2/L[$, we define 
\begin{align}\label{eq:def-mat}
M\eqdef& \begin{pmatrix} (1+a)\Id-\gamma(1+b)\nabla_x^2f(\xsol,\theta) &-a\Id+\gamma b\nabla_x^2f(\xsol,\theta) \\ \Id  & 
		0\end{pmatrix},\\
	=& \begin{pmatrix} (a-b)\Id+(1+b)G_{\theta} &-(a-b)\Id-bG_{\theta} \\ \Id  & 
		0\end{pmatrix},
\end{align}
where $G_{\theta}\eqdef \Id-\gamma\nabla_x^2f(\xsol,\theta).$

 Let us observe that for $\gamma\in]0,2/L[, G_{\theta}$ has eigenvalues in $]-1,1[$ and if $\gamma\in[0,1/L[, G_{\theta}$ has eigenvalues in $[0,1[$. Therefore, let us denote by $\etam_{\theta}$ and $\etaM_{\theta}$ the smallest and the largest eigenvalue of $G_{\theta}$. 


We will need the following premise to pursue our study. 
\begin{premise}\label{prem_C} $~$
\begin{enumerate}[label=(C.\arabic*)]
 \item \label{prem_C1} There exists $a,b\in [0,1]$ and $\gamma\in]0,\frac{2}{L}[$ such that the sequences $\ak \to a$, $\bk \to b$ and $\gak\to \gamma.$ 
 \item\label{prem_C2} Given any limits point $a,b\in[0,1[^2$ of the sequence $\seq{\ak},\seq{\bk}$ respectively the following holds:  
 \[
 \frac{2\pa{b-a}-1}{1+2b}<\etam_{\theta}.
 \]
\end{enumerate}
\end{premise}
\begin{remark} Premise~\ref{prem_C1} suppose that  the sequence of inertial parameters $\seq{\ak}, \seq{\bk}$ and stepsize $\seq{\gak}$  have  limits, while Premise~\ref{prem_C2} is an hypothesis on the link between the limits $a,b$ and the lowest eigenvalue of $G_{\theta}$. This premise is crucial to get the linear convergence rates.  
\end{remark}


We have the following proposition.
\begin{proposition}[Local linear convergence]\label{pro:local-lin} Let us consider the optimization problem \eqref{eq:param-optim} for all $\theta\in\Theta$ solved with the inertial method \eqref{eq:inertial-methods}. Under the premise~\ref{prem_A}, \ref{prem_B}, \ref{prem_C} and the strong convexity hypothesis with respect to $x$, we have  that

\begin{enumerate}
\item\label{pro:local-lin1}  the sequence of matrices $\seq{M_k}$ converges to the matrice $M$ defined in \eqref{eq:def-mat},

\item \label{pro:local-lin2} the spectral radius of $M$ is such that $\rho(M)<1,$

\item\label{pro:local-lin3} for any $\rho\in[\rho(M),1[,$ there exists $K>0$ large enough and a constant $C>0$ such that for all $k\geq K,$ it holds 
\begin{centerbox}{black}{}y
\begin{equation}\label{eq:local-lin}
	\normm{X_k(\theta)-X^{\star}(\theta)}\leq C\rho^{k-K}\normm{X_K(\theta)-X^{\star}(\theta)}.
\end{equation}
\end{centerbox}

\end{enumerate}
\end{proposition}
\begin{proof}$~$
The proof can be found in Section~\ref{prf:pro:local-lin}.
\end{proof}



 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 